{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V6E1"},"accelerator":"TPU","kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":119261,"databundleVersionId":14363498,"sourceType":"competition"},{"sourceId":7045423,"sourceType":"datasetVersion","datasetId":4054119,"isSourceIdPinned":false}],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-Domain GRPO Training with Gemma 3 (1B) using Tunix\n\nThis notebook presents a **complete, end-to-end training pipeline** for teaching a **Gemma 3 (1B)** language model to **explicitly show its reasoning** before producing a final answer.\n\nThe core objective is not just answer accuracy, but **reasoning transparency**.  \nThe model is trained to consistently generate outputs in the following structured format:\n\n```\n\n<reasoning>\nstep-by-step reasoning trace\n</reasoning>\n<answer>\nfinal answer\n</answer>\n```\n\nTraining is performed using **Group Relative Policy Optimization (GRPO)** via **Tunix**, Google‚Äôs JAX-native post-training library. GRPO enables stable reinforcement-learning‚Äìstyle updates without requiring a separate value model, making it well-suited for **small models and limited TPU budgets**.\n\n---\n\n## What This Notebook Demonstrates\n\n* Fine-tuning **Gemma 3 (1B)** on a **single Kaggle TPU session**\n* Multi-domain reasoning training across:\n\n  * Math\n  * Logic\n  * Basic science\n  * Coding (non-executed, verifiable by structure)\n* Strict enforcement of **reasoning + answer separation**\n* Reward shaping for:\n\n  * Format compliance\n  * Refusal avoidance\n  * Correctness (where applicable)\n  * Conciseness and termination discipline\n* A **phase-based curriculum** inspired by Open-R1 / R1-style training\n\nThis notebook is intentionally written to be:\n\n* **Minimal** (no YAML, no hidden configs)\n* **Debuggable** (explicit reward functions)\n* **Reproducible** (single-session training)\n* **Notebook-native** (no external orchestration)\n\n---\n\n## Scope and Constraints\n\n* **Model**: Gemma 3 (1B-IT)\n* **Framework**: Tunix (JAX)\n* **Hardware**: Kaggle TPU (single session)\n* **Max output length**: < 1K tokens\n* **Language**: English only\n* **No tool use or code execution during training**\n\nCoding tasks are evaluated **without running unit tests**. Instead, the model is rewarded for producing **well-formed, logically correct Python code** that follows the required format, aligning with the hackathon‚Äôs LLM-as-a-judge evaluation setup.\n\n---\n\n## Why This Matters\n\nLarge reasoning models are expensive to train.\nThis notebook shows that **even a 1B-parameter model** can be taught to reason more transparently using:\n\n* Careful reward design\n* Curriculum-style training\n* Strong format constraints\n\nThe result is a small, efficient model that **thinks before it answers**‚Äîmaking reasoning more accessible, interpretable, and reproducible.","metadata":{"id":"abdhOBYHqYz6"}},{"cell_type":"code","source":"import os\nos.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Install necessary libraries","metadata":{"id":"afofSj37qYz6"}},{"cell_type":"code","source":"!pip install -q kagglehub\n\n!pip install -q ipywidgets\n\n!pip install -q tensorflow\n!pip install -q tensorflow_datasets\n!pip install -q tensorboardX\n!pip install -q transformers\n!pip install -q grain\n# !pip install \"google-tunix[prod]==0.1.5\"\n\n!pip install -q git+https://github.com/google/tunix\n!pip install -q git+https://github.com/google/qwix\n\n!pip uninstall -q -y flax\n# !pip install -U flax\n!pip install flax==0.12.0\n\n!pip install -q datasets wandb==0.22.0","metadata":{"id":"Z03GnyApTn1j","outputId":"ccd485a1-84b1-4d9f-bf45-4bf4a4c8ee7b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb, os\nfrom kaggle_secrets import UserSecretsClient\nos.environ['WANDB_API_KEY'] = UserSecretsClient().get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"LnF9ZACiTn1k"}},{"cell_type":"code","source":"import functools\nimport gc\nimport os\nfrom pprint import pprint\nimport re\n\nimport csv\nimport shutil\n\nfrom flax import nnx\nimport grain\nimport humanize\nimport jax\nimport jax.numpy as jnp\nimport kagglehub\nimport optax\nfrom orbax import checkpoint as ocp\nfrom pathlib import Path\nimport qwix\nimport tensorflow_datasets as tfds\nfrom tqdm.auto import tqdm\nfrom tunix.generate import sampler as sampler_lib\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\nfrom tunix.models.gemma3 import params\nfrom tunix.models.gemma3 import model\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.sft import metrics_logger\nfrom datasets import load_dataset","metadata":{"id":"McTNo_r8Tn1k","outputId":"67f038b8-509b-46ed-b027-9b72ed7b628c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hyperparameters\n\nLet's define the configuration we are going to use. Note that this is by no\nmeans a \"perfect\" set of hyperparameters. To get good results, you might have\nto train the model for longer.","metadata":{"id":"Eu_NI9nHTn1k"}},{"cell_type":"code","source":"# ====== Data ======\nTRAIN_DATA_DIR = \"./data/train\"\nTEST_DATA_DIR = \"./data/test\"\nTRAIN_FRACTION = 1.0\n\n# ====== LoRA ======\nRANK = 64\nALPHA = 64.0\n\n# ====== Sharding ======\nMESH = [(1, 4), (\"fsdp\", \"tp\")]\n\n# ====== GRPO ======\n# === Generation during GRPO training ===\nMAX_PROMPT_LENGTH = 256\nTOTAL_GENERATION_STEPS = 512\n# Important to keep a high-ish temperature for varied, diverse responses during\n# training.\nTEMPERATURE = 0.9\nTOP_P = 1.0\nTOP_K = 50\n# The number of times the policy generates multiple responses for a given prompt\n# within a single training step. This corresponds to `G` in Algorithm 1 in the\n# paper. The \"group\" in GRPO comes from here.\nNUM_GENERATIONS = 4\n\n# === other GRPO configs ===\n# The number of iterations per batch (ùúá in GRPO algo 1).\nNUM_ITERATIONS = 1\n# The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n# Important to keep a high enough value for this, otherwise, the KL divergence\n# can increase unchecked.\nBETA = 0.08\n# Epsilon value for clipping (ùúÄ in GRPO loss in paper). Similar to PPO, for\n# stable updates.\nEPSILON = 0.2\n\n# ====== Training ======\nTRAIN_MICRO_BATCH_SIZE = 4\n# Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n# NUM_BATCHES = 3738\nNUM_BATCHES = 3\n# Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n# increased to a max. of 330 (if batch size is 4).\nNUM_TEST_BATCHES = 100\n\nEVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\nNUM_EPOCHS = 1  # can potentially train for more epochs\n\n# Number of training steps.\nMAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n\n# === AdamW, warmup, cosine scheduler ===\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\n# == Cosine decay with warmup scheduler ==\n# Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n# steps, and then gradually decrease the learning rate to 0 using cosine\n# scheduler.\nWARMUP_STEPS = 0.1 * MAX_STEPS\n# == Grad clipping ==\n# Grad clipping to prevent large gradients. Found this\n# important to keep KL divergence in check.\nMAX_GRAD_NORM = 0.1\n\n# Checkpoint saving\nINTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\nCKPT_DIR = \"/tmp/content/ckpts/\"\nSAVE_INTERVAL_STEPS = 500\nMAX_TO_KEEP = 4\n\n# ====== Inference ======\nGENERATION_CONFIGS = {\n    # greedy search\n    \"greedy\": {\"temperature\": 1e-4, \"top_k\": 1, \"top_p\": 1.0},\n    # some randomness\n    \"standard\": {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n    # liberal\n    \"liberal\": {\"temperature\": 0.85, \"top_k\": 2000, \"top_p\": 1.0},\n}","metadata":{"id":"ZPPKme47Tn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Utility functions","metadata":{"id":"ngjtE-63Tn1k"}},{"cell_type":"code","source":"def show_hbm_usage():\n  \"\"\"Displays memory usage per device.\"\"\"\n  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n\n  for d in jax.local_devices():\n    stats = d.memory_stats()\n    used = stats[\"bytes_in_use\"]\n    limit = stats[\"bytes_limit\"]\n    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")","metadata":{"id":"wjMFOr7aTn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data preprocessing\n\nFirst, let's define some special tokens. We instruct the model to first reason\nbetween the `<reasoning>` and `</reasoning>` tokens. After\nreasoning, we expect it to provide the answer between the `<answer>` and\n`</answer>` tokens.","metadata":{"id":"6BtpYMlaTn1k"}},{"cell_type":"code","source":"REASONING_START = \"<reasoning>\"\nREASONING_END = \"</reasoning>\"\nANSWER_START = \"<answer>\"\nANSWER_END = \"</answer>\"\n\nSYSTEM_PROMPT = f\"\"\"\nYou are given a problem.\n\nThink step by step and write your reasoning between\n{REASONING_START} and {REASONING_END}.\n\nThen write the final answer as a single value between\n{ANSWER_START} and {ANSWER_END}.\n\nDo not write anything outside these tags.\n\"\"\".strip()\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\"\"\"","metadata":{"id":"h6RGv1kSTn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We use OpenAI's [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k), which comprises grade school math word problems.","metadata":{"id":"WASP9N5JTn1k"}},{"cell_type":"code","source":"from datasets import load_dataset\nimport grain\nimport os\nimport time\n\n# =========================\n# GSM8K answer extractor\n# =========================\ndef extract_hash_answer(text: str) -> str | None:\n    if not isinstance(text, str):\n        return None\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[-1].strip()\n\n\n# =========================\n# MBPP test_list fixer\n# =========================\ndef build_mbpp_tests(x):\n    tests = x.get(\"test_list\", [])\n    if not isinstance(tests, list):\n        return []\n\n    test_cases = []\n    for t in tests:\n        if isinstance(t, dict) and \"input\" in t and \"output\" in t:\n            test_cases.append({\n                \"input\": t[\"input\"],\n                \"output\": t[\"output\"],\n            })\n    return test_cases\n\n\n# =========================\n# DATASET BUILDER\n# =========================\ndef get_dataset(split=\"train\") -> grain.MapDataset:\n    print(f\"\\nüöÄ Building mixed dataset | split = {split}\")\n    t0 = time.time()\n\n    os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n    os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n\n    # ============\n    # GSM8K (MATH)\n    # ============\n    print(\"üì• Loading GSM8K...\")\n    gsm = load_dataset(\"openai/gsm8k\", \"main\", split=split)\n    print(f\"‚úÖ GSM8K loaded ({len(gsm)} samples)\")\n\n    gsm_ds = (\n        grain.MapDataset.source(gsm)\n        .map(lambda x: {\n            \"prompts\": TEMPLATE.format(\n                system_prompt=SYSTEM_PROMPT,\n                question=str(x[\"question\"]),\n            ),\n            \"question\": str(x[\"question\"]),\n            \"answer\": extract_hash_answer(str(x[\"answer\"])),\n            \"domain\": \"math\",\n        })\n        .filter(lambda x: x[\"answer\"] is not None)\n    )\n\n    print(\"üß† GSM8K pipeline built\")\n\n    # ============\n    # MBPP (CODE) ‚Äî FIXED\n    # ============\n    print(\"üì• Loading MBPP...\")\n    mbpp = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\", split=split)\n    print(f\"‚úÖ MBPP loaded ({len(mbpp)} samples)\")\n\n    def mbpp_map(x):\n        test_cases = build_mbpp_tests(x)\n        if not test_cases:\n            return None  # üö® skip broken samples safely\n\n        return {\n            \"prompts\": TEMPLATE.format(\n                system_prompt=SYSTEM_PROMPT,\n                question=str(x[\"text\"]),\n            ),\n            \"question\": str(x[\"text\"]),\n            \"answer\": None,\n            \"verification_info\": {\n                \"language\": \"python\",\n                \"test_cases\": test_cases,\n            },\n            \"domain\": \"code\",\n        }\n\n    mbpp_ds = (\n        grain.MapDataset.source(mbpp)\n        .map(mbpp_map)\n        .filter(lambda x: x is not None)\n    )\n\n    print(mbpp_ds[:1])\n    print(\"üß† MBPP pipeline built (SAFE)\")\n\n    # ============\n    # ARC Easy (SCIENCE)\n    # ============\n    print(\"üì• Loading ARC Easy...\")\n    arc = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\", split=split)\n    print(f\"‚úÖ ARC Easy loaded ({len(arc)} samples)\")\n\n    def arc_prompt(x):\n        choices = \"\\n\".join(\n            f\"{l}. {t}\"\n            for l, t in zip(x[\"choices\"][\"label\"], x[\"choices\"][\"text\"])\n        )\n        return f\"{x['question']}\\n\\nChoices:\\n{choices}\"\n\n    arc_ds = grain.MapDataset.source(arc).map(lambda x: {\n        \"prompts\": TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=arc_prompt(x),\n        ),\n        \"question\": x[\"question\"],\n        \"answer\": x[\"answerKey\"],\n        \"domain\": \"science\",\n    })\n\n    print(\"üß† ARC pipeline built\")\n\n    # =================\n    # StrategyQA (LOGIC)\n    # =================\n    print(\"üì• Loading StrategyQA...\")\n    strategyqa = load_dataset(\"ChilleD/StrategyQA\", split=split)\n    print(f\"‚úÖ StrategyQA loaded ({len(strategyqa)} samples)\")\n\n    def strategyqa_prompt(x):\n        return f\"Question: {x['question']}\\nAnswer yes or no.\"\n\n    strategyqa_ds = grain.MapDataset.source(strategyqa).map(lambda x: {\n        \"prompts\": TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=strategyqa_prompt(x),\n        ),\n        \"question\": x[\"question\"],\n        \"answer\": \"yes\" if x[\"answer\"] else \"no\",\n        \"domain\": \"logic\",\n    })\n\n    print(\"üß† StrategyQA pipeline built\")\n\n    # ============\n    # MIX\n    # ============\n    print(\"üß™ Mixing datasets...\")\n    mixed = (\n        grain.MapDataset.mix(\n            datasets=[gsm_ds, mbpp_ds, arc_ds, strategyqa_ds],\n            weights=[0.35, 0.30, 0.20, 0.15],\n        )\n        .shuffle(seed=42)\n    )\n\n    print(\"‚úÖ Mixed dataset pipeline ready\")\n    print(f\"‚è±Ô∏è Total build time: {time.time() - t0:.2f}s\")\n\n    return mixed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We split the dataset set into train and test sets as usual.","metadata":{"id":"uDwobMu_okwv"}},{"cell_type":"code","source":"print(\"Using data source: huggingface (mixed domains)\")\n\nimport os\nimport time\nimport grain\nimport itertools\nfrom collections import Counter\n\nos.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\nos.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n\nt0 = time.time()\n\n# =====================\n# TRAIN DATASET (STREAMING)\n# =====================\nprint(\"\\nüöß Building TRAIN dataset (STREAMING)...\")\ntrain_ds_raw = get_dataset(\"train\")\n\nprint(\"üß™ Sampling ONE TRAIN example (safe)...\")\nsample = next(iter(train_ds_raw))\nprint(\"‚úÖ TRAIN sample loaded\")\nprint(\"   domain:\", sample[\"domain\"])\nprint(\"   question (truncated):\", sample[\"question\"][:80])\n\n# ---- DEBUG: domain mix (small prefix only)\nprint(\"\\nüîç Checking TRAIN domain mix (first 300 samples)...\")\ntrain_domain_counter = Counter(\n    ex[\"domain\"] for ex in itertools.islice(train_ds_raw, 3000)\n)\nprint(\"   Domain counts:\", train_domain_counter)\n\n# ---- STREAMING BATCHED DATASET (THIS IS WHAT GRPO USES)\nprint(\"\\nüì¶ Creating STREAMING TRAIN batches...\")\ntrain_dataset_for_grpo = (\n    train_ds_raw\n    .to_iter_dataset()\n    .batch(TRAIN_MICRO_BATCH_SIZE)\n)\n\nprint(\"‚úÖ TRAIN streaming dataset ready\")\n\n# =====================\n# TEST DATASET (STREAMING)\n# =====================\nprint(\"\\nüöß Building TEST dataset (STREAMING)...\")\ntest_ds_raw = get_dataset(\"test\")\n\nprint(\"üß™ Sampling ONE TEST example (safe)...\")\nsample = next(iter(test_ds_raw))\nprint(\"‚úÖ TEST sample loaded\")\nprint(\"   domain:\", sample[\"domain\"])\nprint(\"   question (truncated):\", sample[\"question\"][:80])\n\n# ---- DEBUG: domain mix\nprint(\"\\nüîç Checking TEST domain mix (first 300 samples)...\")\ntest_domain_counter = Counter(\n    ex[\"domain\"] for ex in itertools.islice(test_ds_raw, 300)\n)\nprint(\"   Domain counts:\", test_domain_counter)\n\n# ---- STREAMING BATCHED TEST DATASET\nprint(\"\\nüì¶ Creating STREAMING TEST batches...\")\ntest_dataset_for_grpo = (\n    test_ds_raw\n    .to_iter_dataset()\n    .batch(TRAIN_MICRO_BATCH_SIZE)\n)\n\nprint(\"‚úÖ TEST streaming dataset ready\")\n\n# =====================\n# SUMMARY\n# =====================\nprint(\"\\nüìä DATASET STATUS\")\nprint(\"   TRAIN: pure streaming (MBPP preserved)\")\nprint(\"   TEST:  pure streaming (MBPP preserved)\")\nprint(f\"‚è±Ô∏è Total setup time: {time.time() - t0:.2f}s\")","metadata":{"id":"KXhOL6GyTn1k","outputId":"5e15f893-33eb-42e9-f4bd-20be01f2314a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's see how one batch of the training dataset looks like!\n","metadata":{"id":"k7n8L0VzTn1k"}},{"cell_type":"code","source":"from collections import Counter\nfrom pprint import pprint\n\ndomain_counter = Counter()\n\nN = 50  # number of batches to inspect\nfor i in range(N):\n    batch = train_dataset[i]\n    domains = batch[\"domain\"]   # this is an array/list\n    for d in domains:\n        domain_counter[str(d)] += 1\n\nprint(\"Domain counts:\")\npprint(domain_counter)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for ele in train_dataset[:1]:\n  pprint(ele)","metadata":{"id":"5TF-wNQ2Tn1k","outputId":"367cd3ef-9b1c-469d-b50c-71887b040e87","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the policy model and the reference model\n\nThe policy model is the model which is actually trained and whose weights are\nupdated. The reference model is the model with which we compute KL divergence.\nThis is to ensure that the policy updates are not huge and that it does not\ndeviate too much from the reference model.\n\nTypically, the reference model is the base model, and the policy model is the\nsame base model, but with LoRA parameters. Only the LoRA parameters are updated.\n\nNote: We perform full precision (fp32) training. You can, however, leverage\nQwix for QAT.\n\nTo load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\nto have agreed to the Gemma license\n[here](https://www.kaggle.com/models/google/gemma/flax/).","metadata":{"id":"BZxBR7Y_Tn1k"}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\nos.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"KAGGLE_KEY\")\nos.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"KAGGLE_USERNAME\")\n\n# Now this will NOT trigger login\nif \"KAGGLE_USERNAME\" not in os.environ or \"KAGGLE_KEY\" not in os.environ:\n    kagglehub.login()","metadata":{"id":"3GfLHHVYHHKO","outputId":"3290c9e8-2362-44a1-93bf-9a5caa7d201f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This code snippet serves as a workaround to re-save the pre-trained model checkpoint from Kaggle into a local format that is compatible with the [Flax NNX](https://flax.readthedocs.io/en/stable/why.html) library. Because the original checkpoint has parameter names and tensor structures that don't match the target NNX model architecture, it cannot be loaded directly.\n\nWe first load the original weights into a temporary model instance, then extract and re-save the model's state into a new, properly formatted local checkpoint, which can then be successfully loaded by the final sharded NNX model.","metadata":{"id":"nAghcsT_Pmv_"}},{"cell_type":"code","source":"from tunix.models.gemma3 import params\nfrom tunix.models.gemma3 import model as gemma_model\n\nimport jax\nimport jax.numpy as jnp\nimport orbax.checkpoint as ocp\nfrom flax import nnx\n\ndef get_gemma_ref_model(ckpt_path):\n    # ===============================\n    # Device mesh\n    # ===============================\n    mesh = jax.make_mesh(*MESH)\n\n    # ===============================\n    # Model config (‚úÖ CORRECT API)\n    # ===============================\n    model_config = gemma_model.ModelConfig.gemma3_1b_it()\n\n    # ===============================\n    # Build abstract (shape-only) model\n    # ===============================\n    abs_gemma: nnx.Module = nnx.eval_shape(\n        lambda: params.create_model_from_checkpoint(\n            params.GEMMA3_1B_IT,\n            model_config,\n        )\n    )\n\n    # ===============================\n    # Prepare sharded state structure\n    # ===============================\n    abs_state = nnx.state(abs_gemma)\n    pspecs = nnx.get_named_sharding(abs_state, mesh)\n\n    abs_state = jax.tree.map(\n        lambda a, s: jax.ShapeDtypeStruct(\n            a.shape,\n            jnp.bfloat16,\n            sharding=s,\n        ),\n        abs_state,\n        pspecs,\n    )\n\n    # ===============================\n    # Restore checkpoint\n    # ===============================\n    checkpointer = ocp.StandardCheckpointer()\n    restored_params = checkpointer.restore(\n        ckpt_path,\n        target=abs_state,\n    )\n\n    # ===============================\n    # Materialize reference model\n    # ===============================\n    graph_def, _ = nnx.split(abs_gemma)\n    ref_model = nnx.merge(graph_def, restored_params)\n\n    return ref_model, mesh, model_config\n\n\ndef get_lora_model(base_model, mesh):\n    # ===============================\n    # LoRA configuration\n    # ===============================\n    lora_provider = qwix.LoraProvider(\n        module_path=(\n            \".*q_einsum|.*kv_einsum|.*gate_proj|\"\n            \".*down_proj|.*up_proj|.*attn_vec_einsum\"\n        ),\n        rank=RANK,\n        alpha=ALPHA,\n    )\n\n    # ===============================\n    # Apply LoRA\n    # ===============================\n    model_input = base_model.get_model_input()\n    lora_model = qwix.apply_lora_to_model(\n        base_model,\n        lora_provider,\n        **model_input,\n    )\n\n    # ===============================\n    # Re-apply sharding\n    # ===============================\n    with mesh:\n        state = nnx.state(lora_model)\n        pspecs = nnx.get_partition_spec(state)\n        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n        nnx.update(lora_model, sharded_state)\n\n    return lora_model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# Cleanup\n# ===============================\n!rm -rf /tmp/content/intermediate_ckpt/*\n!rm -rf /tmp/content/ckpts/*\n\nimport os, gc, jax\nimport jax.numpy as jnp\nfrom tunix.models.gemma3 import params\nfrom tunix.models.gemma3 import model as gemma_model\nfrom flax import nnx\nimport orbax.checkpoint as ocp\n\nCKPT_PATH = os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n\n# ===============================\n# Model config (CORRECT)\n# ===============================\nmodel_config = gemma_model.ModelConfig.gemma3_1b_it()\n\n# ===============================\n# Load base Gemma 3 1B\n# ===============================\nbase_model = params.create_model_from_checkpoint(\n    params.GEMMA3_1B_IT,\n    model_config,\n)\n\ntokenizer = params.create_tokenizer()\nprint(\"‚úÖ Base Gemma-3 1B loaded\")\n\n# ===============================\n# Save clean base state\n# ===============================\ncheckpointer = ocp.StandardCheckpointer()\n_, base_state = nnx.split(base_model)\n\ncheckpointer.save(CKPT_PATH, base_state)\ncheckpointer.wait_until_finished()\n\nprint(\"‚úÖ Clean base checkpoint saved\")","metadata":{"id":"cIFAxgVOTn1k","outputId":"0235a0e1-9f7d-428c-c16e-0bc9e49c2f2f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Loading and LoRA Application\n\nThese two functions work together to load a base model from a checkpoint and apply a LoRA (Low-Rank Adaptation) layer to it.\n\n* `get_ref_model`: Loads the complete Gemma model from a specified checkpoint path. It uses **JAX sharding** to distribute the model parameters across multiple devices.\n* `get_lora_model`: Takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it's ready for distributed training.","metadata":{"id":"hpgXONuORkkq"}},{"cell_type":"markdown","source":"Now we load reference and policy Gemma models using the Flax NNX library and display their structures.","metadata":{"id":"mgBALRieR6aY"}},{"cell_type":"code","source":"# ===============================\n# Cell 1: Load HF Qwen3-1.7B\n# ===============================\nimport os, gc, torch\nfrom transformers import AutoModelForCausalLM\n\nHF_QWEN_PATH = \"/kaggle/input/qwen-3/transformers/1.7b-base/1\"\n\nprint(\"üì• Loading HF Qwen3-1.7B...\")\nhf_model = AutoModelForCausalLM.from_pretrained(\n    HF_QWEN_PATH,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\",\n    trust_remote_code=True,\n)\n\nhf_state = hf_model.state_dict()\nprint(f\"‚úÖ HF model loaded with {len(hf_state)} tensors\")\n\n\n# ===============================\n# Cell 2: HF ‚Üí Tunix mapping\n# ===============================\nimport jax\nimport jax.numpy as jnp\nimport tunix\nfrom flax import nnx\nfrom tunix.models.qwen3 import model as qwen_model\n\nprint(\"üß± Building Tunix Qwen3-1.7B abstract model...\")\n\nmodel_config = qwen_model.ModelConfig.qwen3_1p7b()\n\nabs_model: nnx.Module = nnx.eval_shape(\n    lambda: qwen_model.Qwen3(\n        config=model_config,\n        rngs=nnx.Rngs(params=jax.random.PRNGKey(0)),\n    )\n)\n\nabs_state = nnx.state(abs_model)\nprint(\"‚úÖ Abstract model ready\")\n\ndef torch_to_jax(x):\n    return jnp.asarray(x.detach().cpu().numpy())\n\ndef map_hf_to_tunix(abs_state, hf_state):\n    mapped = {}\n    for k in abs_state:\n        hf_key = (\n            k.replace(\"layers.\", \"model.layers.\")\n             .replace(\"embedder.input_embedding\", \"model.embed_tokens.weight\")\n             .replace(\"final_norm.w\", \"model.norm.weight\")\n             .replace(\"lm_head.w\", \"lm_head.weight\")\n        )\n        if hf_key in hf_state:\n            mapped[k] = torch_to_jax(hf_state[hf_key])\n    return nnx.freeze(mapped)\n\nprint(\"üîÅ Mapping HF ‚Üí Tunix weights...\")\ntunix_state = map_hf_to_tunix(abs_state, hf_state)\nprint(\"‚úÖ Weight mapping complete\")\n\n\n# ===============================\n# Cell 3: Save Tunix checkpoint\n# ===============================\nimport orbax.checkpoint as ocp\n\nOUT_CKPT_DIR = \"/tmp/qwen3_1p7b_tunix_ckpt\"\nos.makedirs(OUT_CKPT_DIR, exist_ok=True)\n\nprint(\"üíæ Saving Tunix checkpoint...\")\ncheckpointer = ocp.StandardCheckpointer()\ncheckpointer.save(OUT_CKPT_DIR, tunix_state)\ncheckpointer.wait_until_finished()\n\nprint(\"üéâ DONE!\")\nprint(\"üìÇ Tunix checkpoint saved at:\", OUT_CKPT_DIR)\n\n# Cleanup (important on Kaggle)\ndel hf_model, hf_state\ngc.collect()\n\nref_model, mesh, model_config = get_qwen_ref_model(\n    ckpt_path=\"/tmp/qwen3_1p7b_tunix_ckpt\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# Load reference model\n# ===============================\nref_model, mesh, model_config = get_gemma_ref_model(\n    ckpt_path=CKPT_PATH\n)\n\nprint(\"‚úÖ Reference model loaded\")\n\n# ===============================\n# Create LoRA actor\n# ===============================\nlora_policy = get_lora_model(ref_model, mesh)\n\nprint(\"‚úÖ LoRA actor created\")\n\n# ===============================\n# Cleanup memory\n# ===============================\ndel base_model, base_state\ngc.collect()\n\n# ===============================\n# Sanity check\n# ===============================\nactor_params = nnx.state(lora_policy)\nprint(f\"Actor param leaves: {len(jax.tree.leaves(actor_params))}\")\n","metadata":{"id":"kSdZ7aGhTn1k","outputId":"a536819f-dd5f-4e29-8ebe-09234960c114","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define reward functions\n\nWe define four reward functions:\n\n- reward if the format of the output exactly matches the instruction given in\n`TEMPLATE`;\n- reward if the format of the output approximately matches the instruction given\nin `TEMPLATE`;\n- reward if the answer is correct/partially correct;\n- Sometimes, the text between `<answer>`, `</answer>` might not be one\n  number. So, we extract the number, and reward the model if the answer is correct.\n\nThe reward functions are inspired from\n[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb).\n\nFirst off, let's define a RegEx for checking whether the format matches.","metadata":{"id":"zLzR1tJfTn1k"}},{"cell_type":"code","source":"match_format = re.compile(\n    rf\"^[\\s]{{0,}}\"\n    rf\"{REASONING_START}.+?{REASONING_END}.*?\"\n    rf\"{ANSWER_START}(.+?){ANSWER_END}\"\n    rf\"[\\s]{{0,}}$\",\n    flags=re.MULTILINE | re.DOTALL,\n)\n\nmatch_format.search(\n    f\"{REASONING_START}Let me\"\n    f\" think!{REASONING_END}{ANSWER_START}2{ANSWER_END}\",\n)","metadata":{"id":"C7Beft8wTn1k","outputId":"a0ba4233-562d-485d-b9ba-2f22cf2785b4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Give the model a reward of 3 points if the format matches exactly.","metadata":{"id":"Fe1rF15zTn1k"}},{"cell_type":"code","source":"def match_format_exactly(prompts, completions, **kwargs):\n  return [\n      0 if match_format.search(response) is None else 3.0\n      for response in completions\n  ]","metadata":{"id":"_fhQ6pY2Tn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We also reward the model if the format of the output matches partially.","metadata":{"id":"sWdAdUHuTn1k"}},{"cell_type":"code","source":"def match_format_approximately(prompts, completions, answer, **kwargs):\n  question = kwargs[\"question\"]\n  responses = completions\n\n  extracted_responses = [\n      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n      for r in responses\n  ]\n  print(\"START ============================\")\n  print(f\"Question: {question[0]}\")\n  print(f\"Answer: {answer[0]}\")\n  print(f\"Response: {responses[0]}\")\n  print(f\"Extracted: {extracted_responses[0]}\")\n  print(\"END ==============================\")\n    \n  scores = []\n  for completion in completions:\n    score = 0\n    response = completion\n    # Count how many keywords are seen - we penalize if too many!\n    # If we see 1, then plus some points!\n    score += 0.5 if response.count(REASONING_START) == 1 else -0.5\n    score += 0.5 if response.count(REASONING_END) == 1 else -0.5\n    score += 0.5 if response.count(ANSWER_START) == 1 else -0.5\n    score += 0.5 if response.count(ANSWER_END) == 1 else -0.5\n    scores.append(score)\n  return scores","metadata":{"id":"uOhO4f3-Tn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reward the model if the answer is correct. A reward is also given if the answer\ndoes not match exactly, i.e., based on how close the answer is to the correct\nvalue.","metadata":{"id":"A2fNZDgTTn1k"}},{"cell_type":"code","source":"def check_answer(prompts, completions, answer, **kwargs):\n    \"\"\"\n    Domain-aware correctness reward.\n    Works with mixed-domain GRPO batches.\n    \"\"\"\n\n    domains = kwargs[\"domain\"]\n    responses = completions\n\n    # Extract answer text from <answer>...</answer>\n    extracted = [\n        m.group(1).strip() if (m := match_format.search(r)) else None\n        for r in responses\n    ]\n\n    scores = []\n\n    assert len(extracted) == len(answer) == len(domains), (\n        f\"Length mismatch: extracted={len(extracted)}, \"\n        f\"answer={len(answer)}, domain={len(domains)}\"\n    )\n\n    for guess, gold, domain in zip(extracted, answer, domains):\n        # --------------------\n        # Missing answer\n        # --------------------\n        if guess is None:\n            scores.append(0.0)\n            continue\n\n        guess_l = guess.lower().strip()\n        gold_l = str(gold).lower().strip()\n\n        score = 0.0\n\n        # ====================\n        # üî¢ MATH\n        # ====================\n        if domain == \"math\":\n            try:\n                g = float(guess)\n                a = float(gold)\n\n                if g == a:\n                    score = 3.0\n                else:\n                    ratio = g / a if a != 0 else 0.0\n                    if 0.95 <= ratio <= 1.05:\n                        score = 1.5\n                    elif 0.9 <= ratio <= 1.1:\n                        score = 0.5\n                    else:\n                        score = -1.0\n            except:\n                score = -0.5\n\n        # ====================\n        # üî¨ SCIENCE (MCQ)\n        # ====================\n        elif domain == \"science\":\n            if guess_l == gold_l:\n                score = 3.0\n            else:\n                score = -1.0\n\n        # ====================\n        # üß† LOGIC (yes / no)\n        # ====================\n        elif domain == \"logic\":\n            yes_set = {\"yes\", \"true\"}\n            no_set = {\"no\", \"false\"}\n\n            if guess_l in yes_set and gold_l in yes_set:\n                score = 3.0\n            elif guess_l in no_set and gold_l in no_set:\n                score = 3.0\n            else:\n                score = -1.0\n\n        # ====================\n        # üíª CODE (handled elsewhere)\n        # ====================\n        elif domain == \"code\":\n            score = 0.0  # execution reward handles this\n\n        # ====================\n        # ‚ùì Unknown domain\n        # ====================\n        else:\n            score = 0.0\n\n        scores.append(score)\n\n    return scores","metadata":{"id":"S8zcWsmhTn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sometimes, the text between `<answer>` and `</answer>` might not be one\nnumber; it can be a sentence. So, we extract the number and compare the answer.","metadata":{"id":"nIpOVv78Tn1k"}},{"cell_type":"code","source":"match_numbers = re.compile(\n    rf\"{ANSWER_START}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n)\nmatch_numbers.findall(f\"{ANSWER_START}  0.34  {ANSWER_END}\")","metadata":{"id":"NXvRtbk8Tn1k","outputId":"1ab45f0e-d04a-455c-a046-7c7a1ec6ee22","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_numbers(prompts, completions, answer, **kwargs):\n  question = kwargs[\"question\"]\n  responses = completions\n\n  extracted_responses = [\n      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n      for r in responses\n  ]\n\n  scores = []\n  print(\"START ============================\")\n  print(f\"Question: {question[0]}\")\n  print(f\"Answer: {answer[0]}\")\n  print(f\"Response: {responses[0]}\")\n  print(f\"Extracted: {extracted_responses[0]}\")\n  print(\"END ==============================\")\n    \n  for guess, true_answer in zip(extracted_responses, answer):\n    if guess is None:\n      scores.append(0)\n      continue\n    # Convert to numbers\n    try:\n      true_answer = float(true_answer.strip())\n      guess = float(guess.strip())\n      scores.append(1.5 if guess == true_answer else 0.0)\n    except:\n      scores.append(0)\n      continue\n  return scores","metadata":{"id":"oxZQAFKOTn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def punish_refusal(prompts, completions, **kwargs):\n    scores = []\n\n    REFUSAL_PHRASES = [\n        \"please provide the problem\",\n        \"i need the problem\",\n        \"cannot solve without\",\n        \"please provide the reasoning\",\n        \"don‚Äôt provide\",\n        \"i cannot help\",\n        \"cannot answer\",\n        \"unable to solve\",\n        \"please provide\",\n        \"need more information\",\n    ]\n\n    for completion in completions:\n        # -----------------------------\n        # Safe text extraction\n        # -----------------------------\n        if isinstance(completion, str):\n            text = completion.lower()\n        elif isinstance(completion, list) and len(completion) > 0:\n            first = completion[0]\n            if isinstance(first, dict) and \"content\" in first:\n                text = first[\"content\"].lower()\n            else:\n                text = str(completion).lower()\n        else:\n            text = str(completion).lower()\n\n        text = text.strip()\n\n        # -----------------------------\n        # üö® HARD REFUSAL = ABSOLUTE DEATH\n        # -----------------------------\n        if any(p in text for p in REFUSAL_PHRASES):\n            scores.append(-20.0)   # ‚ò¢Ô∏è nuke it from orbit\n            continue\n\n        # -----------------------------\n        # üö´ Empty / ultra-short junk\n        # -----------------------------\n        if len(text) < 15:\n            scores.append(-10.0)\n            continue\n\n        # -----------------------------\n        # ‚úÖ ATTEMPT BONUS (CRITICAL)\n        # -----------------------------\n        attempted = (\n            \"<reasoning>\" in text\n            or \"<answer>\" in text\n            or any(c.isdigit() for c in text)\n        )\n\n        if attempted:\n            scores.append(+1.0)   # üõü trying > refusing\n        else:\n            scores.append(-2.0)   # vague fluff still bad\n\n    return scores\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def penalize_length_and_rambling(prompts, completions, **kwargs):\n    scores = []\n\n    MAX_LEN = 80          # tokens-ish proxy (chars OK too)\n    LENGTH_PENALTY = 4.0  # strong on purpose\n\n    RAMBLE_MARKERS = [\n        \"let's re-read\",\n        \"however\",\n        \"this is not correct\",\n        \"not possible\",\n        \"we are given that\",\n        \"Let's rephrase the problem\",\n        \"let us\",\n    ]\n\n    for completion in completions:\n        # Extract text safely\n        if isinstance(completion, str):\n            text = completion\n        elif isinstance(completion, list) and len(completion) > 0:\n            text = completion[0].get(\"content\", \"\")\n        else:\n            text = str(completion)\n\n        text_lower = text.lower()\n        score = 0.0\n\n        # ===============================\n        # 1Ô∏è‚É£ Length penalty (HUGE)\n        # ===============================\n        length = len(text)\n        if length > MAX_LEN:\n            excess = (length - MAX_LEN) / MAX_LEN\n            score -= LENGTH_PENALTY * excess\n\n        # ===============================\n        # 2Ô∏è‚É£ Talking AFTER answer (ILLEGAL)\n        # ===============================\n        if \"</answer>\" in text:\n            after = text.split(\"</answer>\", 1)[-1].strip()\n            if after:\n                score -= 2.5  # hard slap\n\n        # ===============================\n        # 3Ô∏è‚É£ Rambling / restart detection\n        # ===============================\n        ramble_hits = sum(m in text_lower for m in RAMBLE_MARKERS)\n        score -= 0.5 * ramble_hits\n\n        scores.append(score)\n\n    return scores","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom collections import Counter\n\n# üîë Generic reasoning markers (domain-agnostic)\nREASONING_KEYWORDS = [\n    # logical flow\n    \"because\", \"therefore\", \"thus\", \"hence\", \"so\", \"as a result\",\n    \"this implies\", \"it follows\", \"which means\",\n\n    # reasoning actions\n    \"assume\", \"consider\", \"analyze\", \"evaluate\", \"compare\",\n    \"explain\", \"reason\", \"conclude\", \"determine\",\n\n    # structure\n    \"first\", \"second\", \"next\", \"then\", \"finally\",\n\n    # evidence / grounding\n    \"given\", \"based on\", \"from this\", \"according to\"\n]\n\n\ndef reasoning_quality_reward(prompts, completions, **kwargs):\n    scores = []\n\n    for response in completions:\n        score = 0.0\n        text = response.lower()\n\n        # -----------------------------------\n        # 1Ô∏è‚É£ Require reasoning block\n        # -----------------------------------\n        if \"<reasoning>\" not in text or \"</reasoning>\" not in text:\n            scores.append(-0.4)\n            continue\n\n        m = re.search(r\"<reasoning>(.*?)</reasoning>\", text, re.S)\n        if m is None:\n            scores.append(-0.4)\n            continue\n\n        reasoning = m.group(1).strip()\n\n        # -----------------------------------\n        # 2Ô∏è‚É£ Sentence structure (domain-agnostic)\n        # -----------------------------------\n        sentences = [\n            s.strip() for s in re.split(r\"[.\\n]\", reasoning)\n            if len(s.strip()) > 6\n        ]\n\n        if len(sentences) >= 2:\n            score += 0.15\n        if len(sentences) >= 4:\n            score += 0.15\n        if len(sentences) >= 7:\n            score += 0.1\n\n        # -----------------------------------\n        # 3Ô∏è‚É£ Reasoning keyword usage (NOT spam)\n        # -----------------------------------\n        keyword_hits = sum(reasoning.count(k) for k in REASONING_KEYWORDS)\n\n        if 1 <= keyword_hits <= 5:\n            score += 0.25\n        elif keyword_hits > 8:\n            score -= 0.25  # keyword spam\n\n        # -----------------------------------\n        # 4Ô∏è‚É£ Keyword repetition penalty (ONLY keywords)\n        # -----------------------------------\n        keyword_counts = Counter()\n\n        for kw in REASONING_KEYWORDS:\n            c = reasoning.count(kw)\n            if c > 0:\n                keyword_counts[kw] += c\n\n        if keyword_counts:\n            max_rep = max(keyword_counts.values())\n\n            if max_rep >= 5:\n                score -= 0.5\n            elif max_rep == 4:\n                score -= 0.35\n            elif max_rep == 3:\n                score -= 0.2\n            elif max_rep == 2:\n                score -= 0.1\n\n        # -----------------------------------\n        # 5Ô∏è‚É£ Length sanity (GENERIC, relaxed)\n        # -----------------------------------\n        token_len = len(reasoning.split())\n\n        if token_len < 25:\n            score -= 0.25\n        elif 50 <= token_len <= 250:\n            score += 0.25\n        elif 250 < token_len <= 450:\n            score += 0.15\n        elif token_len > 600:\n            score -= 0.35  # rambling\n\n        # -----------------------------------\n        # 6Ô∏è‚É£ Grounding signals (numbers OR entities OR examples)\n        # -----------------------------------\n        has_numbers = bool(re.search(r\"\\d\", reasoning))\n        has_examples = \"example\" in reasoning or \"for instance\" in reasoning\n        has_entities = bool(re.search(r\"[A-Z][a-z]+\", m.group(1)))\n\n        grounding_hits = sum([has_numbers, has_examples, has_entities])\n\n        if grounding_hits >= 1:\n            score += 0.15\n        if grounding_hits >= 2:\n            score += 0.15\n\n        # -----------------------------------\n        # 7Ô∏è‚É£ Penalize pure fluff phrases\n        # -----------------------------------\n        fluff_phrases = [\n            \"it is obvious\", \"clearly\", \"everyone knows\",\n            \"needless to say\", \"without loss of generality\"\n        ]\n\n        if any(p in reasoning for p in fluff_phrases):\n            score -= 0.3\n\n        # -----------------------------------\n        # 8Ô∏è‚É£ Final clamp (keep GRPO stable)\n        # -----------------------------------\n        score = max(-0.6, min(0.9, score))\n        scores.append(score)\n\n    return scores\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate\n\n\nBefore we train the model, let's evaluate the model on the test set so we can\nsee the improvement post training.\n\nWe evaluate it in two ways:\n\n**Quantitative**\n\n* **Answer Accuracy**: percentage of samples for which the model predicts the\ncorrect final numerical answer  \n* **Answer (Partial) Accuracy**: percentage of samples for which the model\npredicts a final numerical answer such that the \\`model answer / answer\\`\nratio lies between 0.9 and 1.1.  \n* **Format Accuracy**: percentage of samples for which the model outputs the\ncorrect format, i.e., reasoning between the reasoning special tokens, and the\nfinal answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n\n**Qualitative**\n\nWe'll also print outputs for a few given questions so that we can compare the generated output later.\n","metadata":{"id":"AaiYMJxFTn1k"}},{"cell_type":"markdown","source":"We define a helper function to generate an answer, given a prompt.","metadata":{"id":"HAaZ7NjBx99P"}},{"cell_type":"code","source":"def generate(\n    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n):\n  \"\"\"Given prompt, generates text.\"\"\"\n\n  if isinstance(question, str):\n    input_batch = [\n        TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=question,\n        ),\n    ]\n  else:\n    input_batch = [\n        TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=q,\n        )\n        for q in question\n    ]\n\n  out_data = sampler(\n      input_strings=input_batch,\n      max_generation_steps=768,\n      temperature=temperature,\n      top_k=top_k,\n      top_p=top_p,\n      echo=False,\n      seed=seed if seed is not None else None,\n      eos_tokens=[1,106],\n  )\n\n  output = out_data.text\n  if isinstance(question, str):\n    return output[0]\n  return output\n    ","metadata":{"id":"_k58bOicUHJy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Another helper function for evaluation.","metadata":{"id":"zNoa5je7yJOt"}},{"cell_type":"code","source":"def evaluate(\n    dataset,\n    sampler,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.95,\n    num_passes=1,\n    corr_lst=False,\n    make_lst=False,\n):\n  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n\n  response_lst = []\n  corr = 0\n  partially_corr = 0\n  corr_format = 0\n  total = 0\n\n  for batch in tqdm(dataset):\n    answers = batch[\"answer\"]\n    questions = batch[\"question\"]\n\n    multiple_call_responses = [[] for _ in range(len(questions))]\n    for p in range(num_passes):\n      responses = generate(\n          questions, sampler, temperature, top_k, top_p, seed=p\n      )\n      for idx, response in enumerate(responses):\n        multiple_call_responses[idx].append(response)\n\n    for question, multiple_call_response, answer in zip(\n        questions, multiple_call_responses, answers\n    ):\n      # check answer\n      corr_ctr_per_question = 0\n      partially_corr_per_question = 0\n      corr_format_per_question = 0\n      for response in multiple_call_response:\n        extracted_response = (\n            guess.group(1)\n            if (guess := match_numbers.search(response)) is not None\n            else \"-1000000\"\n        )\n        try:\n          if float(extracted_response.strip()) == float(answer.strip()):\n            corr_ctr_per_question += 1\n\n          ratio = float(extracted_response.strip()) / float(answer.strip())\n          if ratio >= 0.9 and ratio <= 1.1:\n            partially_corr_per_question += 1\n        except:\n          print(\"SKIPPED\")\n\n        # check format\n        if match_format.search(response) is not None:\n          corr_format_per_question += 1\n\n        if (\n            corr_ctr_per_question > 0\n            and partially_corr_per_question > 0\n            and corr_format_per_question > 0\n        ):\n          break\n\n      if corr_ctr_per_question > 0:\n        corr += 1\n        if corr_lst and make_lst:\n          response_lst.append((question, answer, multiple_call_response))\n      else:\n        if not corr_lst and make_lst:\n          response_lst.append((question, answer, multiple_call_response))\n      if partially_corr_per_question > 0:\n        partially_corr += 1\n      if corr_format_per_question > 0:\n        corr_format += 1\n\n      total += 1\n      if total % 10 == 0:\n        print(\n            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n        )\n\n  to_return = (\n      corr,\n      total,\n      corr / total * 100,\n      partially_corr / total * 100,\n      corr_format / total * 100,\n  )\n  if make_lst:\n    return to_return, response_lst\n  return to_return","metadata":{"id":"yJo2nuKB-wlw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)","metadata":{"id":"HZMO-KflTn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let's see how the original model does on the test set. You can see the percentages of the mode outputs that are fully correct, partially correct and just correct in format. The following step might take couple of minutes to finish.","metadata":{"id":"UOAQe06DyVlQ"}},{"cell_type":"code","source":"# The evaluation might take up to couple of minutes to finish. Please be patient.\n\n(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n    test_dataset,\n    sampler,\n    **GENERATION_CONFIGS[\"greedy\"],\n)\nprint(\n    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n    f\" {format_accuracy=}%\"\n)","metadata":{"id":"YQM-tzXWUmoE","outputId":"d125e6b1-f940-44b3-a05d-f837d2299f49","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train\n\nLet's set up all the configs first - checkpointing, metric logging and training.\nWe then train the model.","metadata":{"id":"-CmB2ZT9Tn1l"}},{"cell_type":"code","source":"# ===============================\n# Checkpointing options\n# ===============================\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS,\n    max_to_keep=MAX_TO_KEEP,\n)\n\n# ===============================\n# Metrics logger options (NEW API)\n# ===============================\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_dir=\"/tmp/content/tmp/tensorboard/grpo\",\n    project_name=\"tunix-grpo\",\n    run_name=\"gemma3-1b-grpo\",\n    flush_every_n_steps=20,\n)\n","metadata":{"id":"mHzdsYsGTn1l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimizer, learning rate scheduler, gradient clipping\noptimizer = optax.adamw(\n    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=LEARNING_RATE,\n        warmup_steps=WARMUP_STEPS,\n        decay_steps=MAX_STEPS,\n        end_value=0.0,\n    ),\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\nif MAX_GRAD_NORM is not None:\n  optimizer = optax.chain(\n      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n      optimizer,\n  )\n    ","metadata":{"id":"YWvBkWBsruom","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training config\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        # metrics logging\n        metrics_logging_options=metrics_logging_options,\n        # checkpoint saving\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=checkpointing_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n        eos_tokens=[1,106],\n    ),\n)\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n)","metadata":{"id":"_6VxFW1ZTn1l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Setting Up the GRPO Trainer\n\nNow we initialize our system for training. First, we create an `RLCluster` instance, which brings together the **policy model (`actor`)**, a **reference model (`reference`)**, and a **tokenizer**. Our `actor` is a trainable LoRA model, while the `reference` is a fixed base model that we use to guide the training.\n\nWe then create a `GRPOLearner`, the specialized trainer that uses a list of **reward functions** to evaluate and optimize the model's output, completing the RL training setup.\n\nTunix trainers are integrated with [Weights & Biases](https://wandb.ai/) to help you visualize the training progress. You can choose how you want to use it:\n\n**Option 1 (Type 1)**: If you're running a quick experiment or just testing things out, choose this. It creates a temporary, private dashboard right in your browser without requiring you to log in or create an account.\n\n**Option 2 (Type 2)**: If you have an existing W&B account and want to save your project's history to your personal dashboard, choose this. You'll be prompted to enter your API key or log in.","metadata":{"id":"z4yJWiElSmOy"}},{"cell_type":"code","source":"# RL cluster\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n# GRPO Trainer\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns = [\n        punish_refusal,             # üö® hard negative\n        match_format_exactly,       # üö™ gate\n        match_format_approximately, # üß≠ soft format\n        check_answer,               # üéØ correctness\n        penalize_length_and_rambling # üî• TERMINATION PRESSURE\n    ],\n    algo_config=grpo_config,\n)","metadata":{"id":"OIe1lO08Tn1l","outputId":"017a2a3f-d9fd-4ac8-87f5-760e2586dfc9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The first couple of training step might take up to 5 minutes to finish. Please be patient. If you experience long training steps, e.g. >10 minutes per step, please open a bug. Really appreciated!","metadata":{"id":"e8b71ed5"}},{"cell_type":"markdown","source":"\n---\n\n# üß† Training Strategy: Phases, Domains, and Rewards (Tunix + Gemma)\n\nThis project trains a **Gemma3-1B + LoRA** model using **GRPO (Tunix)** with a **curriculum-based reinforcement learning strategy**.\nThe key idea is to **separate behavioral alignment from task competence**, and only introduce domain complexity after the model reliably follows the output contract.\n\n---\n\n## üéØ Core Principle\n\n> **Reinforcement Learning teaches behavior, not skills.**\n> Skills come from the base model and data; RL enforces *how* the model responds.\n\nTherefore:\n\n* Early phases focus on **format, discipline, and attempt behavior**\n* Later phases introduce **correctness**\n* Domain diversity is introduced **only after behavior stabilizes**\n\n---\n\n## ü•á Phase 1 ‚Äî Behavioral Alignment (Format & Discipline)\n\n### üéØ Objective\n\nTrain the model to:\n\n* Always attempt an answer\n* Always output **only** inside:\n\n  ```text\n  <reasoning>...</reasoning>\n  <answer>...</answer>\n  ```\n* Never refuse\n* Avoid rambling or extraneous text\n\n**Correctness is NOT optimized in this phase.**\n\n---\n\n### üìö Dataset\n\n* **GSM8K (subset)**\n* Random 20‚Äì30% slice, or filtered for short questions\n\nReason:\n\n* GSM8K prompts are short and structured\n* Easy to parse\n* Domain content is irrelevant at this stage\n\n---\n\n### üèÜ Reward Functions (Phase 1)\n\n```python\nreward_fns = [\n    punish_refusal,        # hard negative for refusing / dodging\n    strict_format_gate,    # requires valid <answer> tag\n    light_length_penalty   # discourages rambling\n]\n```\n\n**No correctness rewards.**\n**No numeric checks.**\n**No partial credit.**\n\nFormat is a **gate**, not a prize.\n\n---\n\n### üìä Metrics to Track\n\n* **Answer extraction rate** (primary KPI)\n* Refusal rate\n* Average output length\n\n### ‚úÖ Phase Completion Criteria\n\n* ‚â• 95% successful `<answer>` extraction\n* Near-zero refusal\n* Stable output length\n\n---\n\n## ü•à Phase 2 ‚Äî Correctness & Honesty (Single Domain)\n\n### üéØ Objective\n\nTeach the model:\n\n* Be correct when possible\n* Be concise when wrong\n* Do not hallucinate\n\n---\n\n### üìö Dataset\n\n* **Full GSM8K**\n\nThe domain stays fixed to avoid confounding behavior with task switching.\n\n---\n\n### üèÜ Reward Functions (Phase 2)\n\n```python\nreward_fns = [\n    punish_refusal,\n    strict_format_gate,\n    check_answer_strict,    # exact match only (no ratios)\n    penalize_length         # stronger penalty for verbose wrong answers\n]\n```\n\nDesign choices:\n\n* Binary correctness signal\n* No ratio-based partial credit\n* Wrong answers are penalized, especially if verbose\n\n---\n\n### üìä Metrics to Track\n\n* Answer extraction rate (should remain high)\n* Hallucination frequency\n* Average length of wrong answers\n* Accuracy trend (expected to improve slowly)\n\n---\n\n## üü¶ Phase 3 ‚Äî Domain Generalization (Optional but Recommended)\n\n### üéØ Objective\n\nApply the learned **behavioral contract** across domains:\n\n* Code\n* Creative reasoning\n* Science\n* Open-ended QA\n\n---\n\n### üìö Dataset\n\nA **mixed dataset**, e.g.:\n\n| Domain         | Proportion |\n| -------------- | ---------- |\n| GSM8K          | 50%        |\n| Code           | 25%        |\n| QA / Reasoning | 25%        |\n\n---\n\n### üèÜ Reward Functions (Phase 3)\n\n```python\nreward_fns = SAME AS PHASE 2\n```\n\n**Important rule:**\n\n> **Do NOT change rewards when introducing new domains.**\n\nConsistency ensures behavior generalizes.\n\n---\n\n### üìä Metrics to Track\n\n* Format compliance across domains\n* Refusal rate\n* Rambling frequency\n* Qualitative reasoning quality\n\n---\n\n## üîÅ Phase Transitions (Rules)\n\n* ‚ùå Do NOT change dataset and rewards at the same time\n* ‚ùå Do NOT mix domains before Phase 1 stabilizes\n* ‚úÖ Only advance phases after behavior plateaus\n\n---\n\n## üß† Why This Works for Tunix Evaluation\n\n* Enforces the exact output format required by judges\n* Aligns with human + LLM-as-judge evaluation\n* Avoids over-optimizing math (low eval weight)\n* Produces consistent, interpretable reasoning traces\n\n---\n\n## üèÅ Summary\n\n| Phase   | Focus               | Dataset       | Rewards                      |\n| ------- | ------------------- | ------------- | ---------------------------- |\n| Phase 1 | Format & discipline | GSM8K subset  | Format + refusal + brevity   |\n| Phase 2 | Correctness         | Full GSM8K    | Strict correctness + brevity |\n| Phase 3 | Generalization      | Mixed domains | Same as Phase 2              |\n\n---\n\n> **Final takeaway:**\n> First teach the model *how to behave*.\n> Only then ask it to be right ‚Äî everywhere.\n\n---\n\nIf you want next:\n\n* üìà a **phase auto-switching heuristic**\n* üß™ a **live extraction-rate logger**\n* üß† a **judge-aligned evaluation prompt**\n\nsay the word üò§üî•\ndef iterable_slice(dataset, start_frac, end_frac):\n    N = len(dataset)\n    start = int(start_frac * N)\n    end = int(end_frac * N)\n\n    def generator():\n        for i, sample in enumerate(dataset):\n            if i >= end:\n                break\n            if i >= start:\n                yield sample\n\n    return generator()\n","metadata":{}},{"cell_type":"code","source":"with mesh:\n  grpo_trainer.train(train_dataset)","metadata":{"id":"S27XDebYTn1l","outputId":"2869f47f-c7d7-4bcc-b167-f05c77a619fa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate\n\nLet's evaluate our finetuned model!","metadata":{"id":"FzIP8glkTn1l"}},{"cell_type":"code","source":"# Load checkpoint first.\nimport re\n\n# Find the latest checkpoint by listing directories in CKPT_DIR/actor\nactor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\n\nlatest_step = -1\nif os.path.exists(actor_ckpt_dir):\n  for item in os.listdir(actor_ckpt_dir):\n    if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n      step = int(item)\n      if step > latest_step:\n        latest_step = step\n\nif latest_step == -1:\n  raise FileNotFoundError(f\"No checkpoints found in {actor_ckpt_dir}\")\n\nprint(f\"Latest checkpoint step: {latest_step}\")\n\nwandb.init(project='tunix-eval')  # logging bug workaround\n\ntrained_ckpt_path = os.path.join(\n    CKPT_DIR, \"actor\", str(latest_step), \"model_params\"\n)\n\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\ntrained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n\nnnx.update(\n    lora_policy,\n    jax.tree.map(\n        lambda a, b: b,\n        nnx.state(lora_policy, nnx.LoRAParam),\n        trained_lora_params,\n    ),\n)","metadata":{"id":"V-73HfP1Tn1l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)","metadata":{"id":"1vY9kl-ITn1l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The evaluation might take up to couple of minutes to finish. Please be patient.\n(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n    test_dataset,\n    sampler,\n    **GENERATION_CONFIGS[\"greedy\"],\n)\nprint(\n    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n    f\" {format_accuracy=}%\"\n)","metadata":{"id":"nz0q_gGHqYz6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With sufficient training, you should see that the percentages of correct model outputs have clearly gone up, which means our training worked.","metadata":{"id":"s1NMAxMh0H5D"}},{"cell_type":"code","source":"# =====================================\n# SAVE FULL TUNIX-LOADABLE CHECKPOINT\n# =====================================\nimport os\nimport json\nimport orbax.checkpoint as ocp\nfrom flax import nnx\n\nEXPORT_DIR = \"./tunix_full_model\"\nos.makedirs(EXPORT_DIR, exist_ok=True)\n\n# Extract FULL params (base + LoRA merged)\nfull_params = nnx.state(lora_policy)\n\n# Save using Orbax (Tunix-compatible)\nfull_checkpointer = ocp.StandardCheckpointer()\nfull_checkpointer.save(\n    EXPORT_DIR,\n    full_params,\n    force=True,\n)\n\n# Optional but recommended: minimal model card\nmodel_card = {\n    \"model_name\": \"gemma3-1b-grpo-reasoning\",\n    \"base_model\": \"google/gemma-3-1b\",\n    \"framework\": \"tunix / jax / flax-nnx\",\n    \"training_method\": \"GRPO\",\n    \"task\": \"reasoning\"\n}\n\nwith open(os.path.join(EXPORT_DIR, \"model_card.json\"), \"w\") as f:\n    json.dump(model_card, f, indent=2)\n\nprint(\"‚úÖ Full Tunix model saved at:\", EXPORT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================\n# UPLOAD FULL MODEL TO KAGGLE (MODEL)\n# =====================================\n\nimport json\nimport subprocess\n\nMODEL_SLUG = \"gemma3-1b-grpo-reasoning-tunix\"\n\nOWNER = subprocess.check_output([\"kaggle\", \"whoami\"]).decode().strip()\nMODEL_ID = f\"{OWNER}/{MODEL_SLUG}\"\n\nprint(f\"üì¶ Uploading Kaggle model: {MODEL_ID}\")\n\n# Kaggle model metadata\nmetadata = {\n    \"title\": \"Gemma3 1B GRPO Reasoning (Tunix)\",\n    \"id\": MODEL_ID,\n    \"licenses\": [{\"name\": \"apache-2.0\"}]\n}\n\nwith open(\"model-metadata.json\", \"w\") as f:\n    json.dump(metadata, f, indent=2)\n\n# Create model (no-op if already exists)\n!kaggle models create -p . --metadata model-metadata.json || true\n\n# Upload model version\n!kaggle models versions create \\\n    -p tunix_full_model \\\n    -m \"Full Gemma3 1B model trained with GRPO (Tunix compatible)\"\n\nprint(\"üéâ DONE\")\nprint(\"‚úÖ SUBMIT THIS MODEL NAME:\")\nprint(MODEL_ID)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}